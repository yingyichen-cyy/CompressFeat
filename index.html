 <!DOCTYPE html>
<html lang="en">
<head>
  <title>CompressFeat</title>
  <meta name="description" content="Project page for Compressing Features for Learning with Noisy Labels.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Compressing Features for Learning with Noisy Labels</h1>
    <h4>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2022</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-2"></div>
    <div class="col-xs-12 col-md-8">
      <h4>
        <a href="https://github.com/yingyichen-cyy"><nobr>Yingyi Chen</nobr></a><sup>1</sup> &emsp;
        <a href="http://hushell.github.io/"><nobr>Shell Xu Hu</nobr></a><sup>2</sup> &emsp;
        <a href="https://xishen0220.github.io/"><nobr>Xi Shen</nobr></a><sup>3</sup> &emsp;
        <a href="https://myweb.cuhk.edu.cn/chunrongai"><nobr>Chunrong Ai</nobr></a><sup>4</sup> &emsp;
        <a href="https://www.esat.kuleuven.be/sista/members/suykens.html"><nobr>Johan A.K. Suykens</nobr></a><sup>1</sup>
        
      </h4>

      <sup>1</sup> ESAT-STADIUS, KU Leuven, Leuven &emsp;
      <sup>2</sup> Samsung AI Center, Cambrige <br /> 
      <sup>3</sup> Tencent AI, Shenzhen &emsp;
      <sup>4</sup> CUHK, Shenzhen
    </div>

    <div class="hidden-xs hidden-sm col-md-1" style="text-align:center; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2006.11132.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i> PDF</a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:center; margin-left:0px;">
      <a href="https://github.com/yingyichen-cyy/Nested-Co-teaching" style="color:inherit">
        <i class="fa fa-github fa-4x"></i> GitHub</a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.jpg" alt="teaser.jpg" class="text-center" style="width: 100%; max-width: 1100px">
  <p>Comparisons of regression between standard MLP and MLP trained with Nested Dropout and Dropout on a synthetic noisy label dataset. 
  (a) MLP with standard training; 
  (b-d) predictions of MLP+Nested using only the first <img src="http://latex.codecogs.com/svg.latex?k\in\{1,10,100\}" alt="k\in\{1,10,100\}" border="0"/> channels;
  (e-h) predictions of MLP+Dropout with drop ratio <img src="http://latex.codecogs.com/svg.latex?p_{\text{drop}}\in\{0.9,0.7,0.5,0.3\}" alt="p_{\text{drop}}\in\{0.9,0.7,0.5,0.3\}" border="0"/>.
  </p>
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://arxiv.org/abs/2006.11132">Paper</a>
    <a class="label label-info" href="https://github.com/yingyichen-cyy/Nested-Co-teaching">Code</a>
    <a class="label label-info" href="resrc/ref.bib">BibTeX</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    Supervised learning can be viewed as distilling relevant information from input data into feature representations.
    This process becomes difficult when supervision is noisy as the distilled information might not be relevant.
    In fact, recent research shows that networks can easily overfit all labels including those that are corrupted, and hence can hardly generalize to clean datasets.
    In this paper, we focus on the problem of learning with noisy labels and introduce compression inductive bias to network architectures to alleviate this over-fitting problem.
    More precisely, we revisit one classical  regularization named Dropout
    and its variant Nested Dropout.
    Dropout can serve as a compression constraint for its feature dropping mechanism,
    while Nested Dropout further learns ordered feature representations w.r.t. feature importance.
    Moreover, the trained models with compression regularization are further combined with Co-teaching for performance boost.
    <br /> 
    Theoretically, we conduct bias-variance decomposition of the objective function under compression regularization. 
    We analyze it for both single model and Co-teaching.
    This decomposition provides three insights:
    <i>(i)</i> it shows that over-fitting is indeed an issue in learning with noisy labels; 
    <i>(ii)</i> through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise;
    <i>(iii)</i> it gives explanations on the performance boost brought by incorporating compression regularization into Co-teaching.
    Experiments show that our simple approach can have comparable or even better performance than the state-of-the-art methods on benchmarks with real-world label noise including Clothing1M and ANIMAL-10N.
  </p>

  <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-6">
      <h4><u>Presentation on Nested+Co-teaching</u> (7min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/y9zBDioKMM0" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <h3>Method</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <h4 style="margin-right: 20%"><u>Overview</u></h4>
    </div>
    <div class="col-xs-6">
      <h4><u></u></h4>
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <img src="resrc/workflow.jpg" alt="workflow.jpg" class="text-center" style="width: 100%; max-width: 900px">
    </div>
    <div class="col-xs-6">
      <div style="width: 90%; max-width: 900px; padding-top:10px">
      <p>Overview of our method. In stage one, the hidden activation <img src="http://latex.codecogs.com/svg.latex?\tilde{Z}" alt="\tilde{Z}" border="0"/> is computed by a feature extractor <img src="http://latex.codecogs.com/svg.latex?f" alt="f" border="0"/>. 
      Dropout/Nested Dropout is applied to <img src="http://latex.codecogs.com/svg.latex?\tilde{Z}" alt="\tilde{Z}" border="0"/> by masking some of the features to zeros, i.e., <img src="http://latex.codecogs.com/svg.latex?Z=M\odot \tilde{Z}" alt="Z=M\odot \tilde{Z}" border="0"/>. 
      The compressed feature <img src="http://latex.codecogs.com/svg.latex?Z" alt="Z" border="0"/> is then fed into the network structure <img src="http://latex.codecogs.com/svg.latex?d" alt="d" border="0"/>, which can simply be a fully connected layer (FC), to perform the final prediction. 
      In stage two, the two base networks are fine-tuned with Co-teaching.</p>
      </div>
    </div>
  </div>

  <h3>Theoretical Analysis</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Bias-variance decomposition of a single network</u></h4>
      <p>Intuitively, the bias term in the equation in Theorem 1 determines how close the average model <img src="http://latex.codecogs.com/svg.latex?\tilde{q}(y|x)" alt="\tilde{q}(y|x)" border="0"/> is to <img src="http://latex.codecogs.com/svg.latex?p(y|x)" alt="p(y|x)" border="0"/> and <img src="http://latex.codecogs.com/svg.latex?p(y|x)" alt="p(y|x)" border="0"/> is the conditional probability for the noisy <img src="http://latex.codecogs.com/svg.latex?Y" alt="Y" border="0"/>, while the variance term promotes a consensus among individual models.
      The variance term also serves as a regularization to combat label noise in the sense that the consensus downweights the influence of the incorrect labels.
      Unlike learning with clean data, we do not expect low bias as it indicates model's over-fitting to label noise.
      Instead, we rely on the variance term and early stopping to provide good training signals.
      </p>
      <img src="resrc/thm1.jpg" alt="thm1.jpg" class="text-center" style="width: 55%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Bias-variance decomposition of our two-stage model</u></h4>
      <p>The condition <img src="http://latex.codecogs.com/svg.latex?\alpha(y|x) \leq 1" alt="\alpha(y|x) \leq 1" border="0"/> in Theorem 2 equals to <img src="http://latex.codecogs.com/svg.latex?q_t(y|x) \geq \log\big[\mathbb{E}_{\tilde{q}(y|x)}\exp[ q_t(y|x)]\big]" alt="q_t(y|x) \geq \log\big[\mathbb{E}_{\tilde{q}(y|x)}\exp[ q_t(y|x)]\big]" border="0"/> where the right-hand side measures the difference between a single training network <img src="http://latex.codecogs.com/svg.latex?\tilde{q}(y|x)" alt="\tilde{q}(y|x)" border="0"/> and the teacher network <img src="http://latex.codecogs.com/svg.latex?q_t(y|x)" alt="q_t(y|x)" border="0"/> of Co-teaching. 
      The larger the difference, the smaller the value, and vise versa.
      Hence, <font color="red">to obtain smaller bias term</font> than that in Theorem 1, the sample selection of Co-teaching only chooses those <img src="http://latex.codecogs.com/svg.latex?(x,y)" alt="(x,y)" border="0"/> with large <img src="http://latex.codecogs.com/svg.latex?q_t(y|x)" alt="q_t(y|x)" border="0"/> so as to meet the condition.
      As <img src="http://latex.codecogs.com/svg.latex?C_1(x,z)\leq 1" alt="C_1(x,z)\leq 1" border="0"/> by definition, if further <img src="http://latex.codecogs.com/svg.latex?\alpha(y|x) \leq C_1(x,z)" alt="\alpha(y|x) \leq C_1(x,z)" border="0"/>, i.e., <img src="http://latex.codecogs.com/svg.latex?q_t(y|x) \geq \log\big[\mathbb{E}_{\tilde{q}(y|x)}\exp[ q_t(y|x)]/C_1(x,z)\big]" alt="q_t(y|x) \geq \log\big[\mathbb{E}_{\tilde{q}(y|x)}\exp[ q_t(y|x)]/C_1(x,z)\big]" border="0"/> with larger right-hand side value, then we will have <font color="red">larger variance term</font> than that in Theorem 1.
      </p>
      <img src="resrc/thm2.jpg" alt="thm2.jpg" class="text-center" style="width: 70%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Conclusion</u></h4>
      <p>Theorem 2 demonstrate that choosing samples with large <img src="http://latex.codecogs.com/svg.latex?q_t(y|x)" alt="q_t(y|x)" border="0"/> during selection of <font color=#00BBFF>Co-teaching</font> leads to <font color="red">smaller bias term and larger variance term</font> than those in Theorem 1.
      That is, the impact of the bias term can be even diminished.
      Consequently, the sample selection mechanism during Co-teaching's cross-update process helps in further preventing networks from over-fitting on noisy labels, thus achieving better performance on clean datasets.
      </p>
  </div>


  <h3>Experiments</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>CIFAR-10 & CIFAR-100 with synthetic label noise</u></h4>
      <p>Test accuracy (%) of state-of-the-art methods under (a) symmetric noise on CIFAR-10 and CIFAR-100, (b) 40% asymmetric noise on CIFAR-10. 
      All approaches are implemented with PreAct ResNet-18 architecture.
      </p>
      <img src="resrc/cifar.jpg" alt="cifar.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Clothing1M with real-world label noise</u></h4>
      <p>Test accuracy (%) of state-of-the-art methods on Clothing1M (noise ratio ∼38%). 
      All approaches are implemented with ResNet-50 architecture.
      Results with ``*" use a balanced subset or a balanced loss.
      </p>
      <img src="resrc/clothing1m.jpg" alt="clothing1m.jpg" class="text-center" style="width: 25%; max-width: 1000px;
      margin-top: 5px">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>ANIMAL-10N with real-world label noise</u></h4>
      <p>Average test accuracy (%) with standard deviation (3 runs) of state-of-the-art methods on ANIMAL-10N (noise ratio ~8%). All approaches are implemented with VGG-19 architecture.
      Results with ``*" use two networks for training.
      </p>
      <img src="resrc/animal.jpg" alt="animal.jpg" class="text-center" style="width: 27%; max-width: 1100px">
  </div>
  
  <h3>Resources</h3>
  <hr/>
  <!-- <div class="row" style="text-align: center">
    <div class="col-xs-0 col-lg-0"></div>
    <div class="col-xs-4 col-lg-4">
      <h4>Paper</h4>
      <a href="https://arxiv.org/abs/2006.11132" style="color:inherit">
        <img src="resrc/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:70%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-4 col-lg-4">
      <h4>Code</h4>
      <a href="https://github.com/monniert/dti-clustering" style="color:inherit;">
        <img src="resrc/github_repo.png" alt="github_repo.png" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-4 col-lg-4">
      <h4>Slides</h4>
      <a href="dtic_long.pptx" style="color:inherit;">
        <img src="resrc/slides.png" alt="slides.png" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-0 col-lg-0"></div>
  </div> -->
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
<!-- @inproceedings{monnier2020dticlustering,
  title={{Deep Transformation-Invariant Clustering}},
  author={Monnier, Tom and Groueix, Thibault and Aubry, Mathieu},
  booktitle={NeurIPS},
  year={2020}, -->
<!-- }</pre> -->
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2104.14575">Monnier et al. - Unsupervised Layered Image Decomposition into Object
        Prototypes (arXiv 2021)</a>
    </li>
  </ul>

  <h4>Previous works on deep transformations</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1908.04725">Deprelle et al. - Learning elementary structures for 3D shape
        generation and matching (NeurIPS 2019)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1806.05228">Groueix et al. - 3D-CODED: 3D Correspondences by Deep Deformation (ECCV
        2018)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1802.05384">Groueix et al. - AtlasNet: A Papier-Mache Approach to Learning 3D
        Surface Generation (CVPR 2018)</a>
    </li>
  </ul>
 -->

  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work is jointly supported by EU: The research leading to these results has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation program / ERC Advanced Grant E-DUALITY (787960). This paper reflects only the authors' views and the Union is not liable for any use that may be made of the contained information.
    Research Council KU Leuven:
    Optimization frameworks for deep kernel machines C14/18/068
    Flemish Government:
    FWO: projects: GOA4917N (Deep Restricted Kernel Machines: Methods and Foundations), PhD/Postdoc grant
    This research received funding from the Flemish Government (AI Research Program).
    Ford KU Leuven Research Alliance Project KUL0076 (Stability analysis and performance improvement of deep reinforcement learning algorithms)
    EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI - Integrating Reasoning, Learning and Optimization)
    Leuven.AI Institute
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
